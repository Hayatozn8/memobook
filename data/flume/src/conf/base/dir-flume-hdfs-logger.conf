# 1. define source, channel, sink
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 2. source
a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /opt/module/apache-flume-1.7.0-bin/upload
a1.sources.r1.fileSuffix = .COMPLETED 
a1.sources.r1.ignorePattern = ([^ ]*\.tmp)


# 3. sink
# 将输出的目的地改为hdfs
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://nn01:9000/flume/%Y%m%d/%H
# 文件的前缀 + 后缀
a1.sinks.k1.hdfs.filePrefix = logs-
#a1.sinks.k1.hdfs.fileSuffix
# 文件的滚动周期，滚动大小，滚动数量
# 滚动大小应该配置成hdfs数据块的大小，而且应该小一点，防止上传时的数据量超过数据块的大小
# 配置周期，防止长时间没有数据时，能够按照日、时等单位进行滚动
# 数量一般会关闭（=0）
a1.sinks.k1.hdfs.rollInterval = 30
a1.sinks.k1.hdfs.rollSize = 134217700
a1.sinks.k1.hdfs.rollCount = 0
# 目录的滚动，可以按照日、时进行目录的滚动，在用hive处理时，相当于数据已经做好了分区，可以直接在hive中使用
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 1
a1.sinks.k1.hdfs.roundUnit = hour
# 使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true
# 积攒多少个 Event 才向 HDFS 输出一次
a1.sinks.k1.hdfs.batchSize = 1000
# 设置文件类型，可支持压缩
a1.sinks.k1.hdfs.fileType = DataStream

# 4. channels
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 5. connect source、channel、sink
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

